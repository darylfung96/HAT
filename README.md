# HAT [[Paper Link]](https://arxiv.org/abs/2205.04437)

### Activating More Pixels in Image Super-Resolution Transformer
Xiangyu Chen, [Xintao Wang](https://scholar.google.com.hk/citations?user=FQgZpQoAAAAJ&hl=en), [Jiantao Zhou](https://scholar.google.com/citations?hl=zh-CN&user=mcROAxAAAAAJ) and [Chao Dong](https://scholar.google.com.hk/citations?user=OSDCB0UAAAAJ&hl=zh-CN)

**The inference results on benchmark datasets are available at
[Google Drive](https://drive.google.com/drive/folders/1t2RdesqRVN7L6vCptneNRcpwZAo-Ub3L?usp=sharing) or [Baidu Netdisk](https://pan.baidu.com/s/1CQtLpty-KyZuqcSznHT_Zw) (access code: 63p5).**

## Dependency
torch>=1.7  
basicsr==1.3.4.9

**Codes and models will be released soon.**  
**This repo is being updated. It is expected to be completed before May 23th.**

<img src="https://raw.githubusercontent.com/chxy95/HAT/master/figures/Performance_comparison.png" width="600"/>
